{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST FFD NN\n",
    "## Classifying MNIST dataset with Feed Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our case we are going to use the MNIST dataset, a standard benchmark for neural networks. For our small network we can get great performance on this dataset. MNIST is  a collection of handwritten digits from 0 to 9, so this is a multiclass classification probllem. Each instance is a 28 by 28 image unwrapped into 784 pixels each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets load our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import _pickle as pkl\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpacks the MNIST data and chunks it into minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_to_bit_vector(labels, nbits):\n",
    "    \"\"\"Returns label in bit vector format\"\"\"\n",
    "    bv = np.zeros((labels.shape[0], nbits))\n",
    "    for i in range(labels.shape[0]):\n",
    "        bv[i, labels[i]] = 1.0\n",
    "        \n",
    "    return bv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_minibatches(data, labels, batch_size, create_bit_vector=False):\n",
    "    N = data.shape[0]\n",
    "    \n",
    "    print(\"Total number of examples: {}\".format(N))\n",
    "    \n",
    "    if N % batch_size != 0:\n",
    "        print(\"create_minibatches(): batch size {} does not\" \\\n",
    "             \"evenly divide number of examples {}\".format(batch_size, N))\n",
    "    \n",
    "    chunked_data = []\n",
    "    chunked_labels = []\n",
    "    idx = 0\n",
    "    \n",
    "    while idx+batch_size <= N:\n",
    "        chunked_data.append(data[idx:idx+batch_size, :])\n",
    "        if not create_bit_vector:\n",
    "            chunked_labels.append(labels[idx:idx+batch_size])\n",
    "        else:            \n",
    "            bv = label_to_bit_vector(labels[idx:idx+batch_size], 10)\n",
    "            chunked_labels.append(bv)\n",
    "        \n",
    "        idx += batch_size\n",
    "        \n",
    "    return chunked_data,chunked_labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = gzip.open('mnist.pkl.gz')\n",
    "train_set, valid_set, test_set = pkl.load(f,encoding='iso-8859-1')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating minibatch of size 100\n",
      "Training:\n",
      "Total number of examples: 50000\n",
      "Testing:\n",
      "Total number of examples: 10000\n",
      "Minibatch of size 100 created\n",
      "Length of training data: 500\n"
     ]
    }
   ],
   "source": [
    "minibatch_size = 100\n",
    "print(\"Creating minibatch of size {}\".format(minibatch_size))\n",
    "print(\"Training:\")\n",
    "train_data, train_labels = create_minibatches(train_set[0], train_set[1],\n",
    "                                             minibatch_size,\n",
    "                                             create_bit_vector=True)\n",
    "print(\"Testing:\")\n",
    "valid_data, valid_labels = create_minibatches(valid_set[0], valid_set[1],\n",
    "                                             minibatch_size,\n",
    "                                             create_bit_vector=True)\n",
    "\n",
    "print(\"Minibatch of size {} created\".format(minibatch_size))\n",
    "print(\"Length of training data:\",len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Plot our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bit_vector_to_label(bv):\n",
    "    label = []\n",
    "    j = 0\n",
    "    for i in bv:\n",
    "        if i != 0:\n",
    "            label.append(str(j))\n",
    "        j += 1\n",
    "    return \",\".join(label)        \n",
    "    \n",
    "def show_a_single_mnist_digit(data):\n",
    "    pixels = data.copy().reshape((28, 28))\n",
    "    plt.imshow(pixels, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "def show_mnist_digits(data,label):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        a) data:\n",
    "                type:     array\n",
    "                contains: array of {nd-array(28x28) representing a digit}\n",
    "        b)label:\n",
    "                type:     array\n",
    "                contains: array of labels corresponding to each item in data in bit vector form\n",
    "    \"\"\"\n",
    "    l = len(data)\n",
    "    \n",
    "    fig =  plt.figure(figsize=(16, ((l//20 + 1) * 3)))\n",
    "    for i,example in enumerate(data):\n",
    "        subplot = fig.add_subplot(l//10 + 1,10,i+1)\n",
    "        pixels = example.copy().reshape((28, 28))\n",
    "        subplot.imshow(pixels, cmap='gray')\n",
    "        subplot.set_title(\"Predicted:{}\".format( (bit_vector_to_label(label[i])) ))\n",
    "        subplot.axis('off')\n",
    "    plt.show()    \n",
    "\n",
    "#show_mnist_digits(train_data[0],train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets impliment our Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions\n",
    "During training we need to compute the activation fuction values for each layer in addition to their derivatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_sigmoid(X, deriv=False):\n",
    "    if not deriv:\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    else:\n",
    "        return f_sigmoid(X)*(1 - f_sigmoid(X))\n",
    "\n",
    "def f_softmax(X):\n",
    "    Z = np.sum(np.exp(X), axis=1)\n",
    "    Z = Z.reshape(Z.shape[0], 1)\n",
    "    return np.exp(X) / Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the network\n",
    "For this case the only hyper parameters are the network size and the minibatch size (default=100). In case of MNIST the input layer has 784 units and the output layer has 10 units. So the network size is specified in the format [784, A, B, 10] where A is the number of units in the first hidden layer and B is that of in second layer. The learning rate selection is also crutial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, size, minibatch_size, is_input=False, is_output=False, activation=f_sigmoid):\n",
    "        self.is_input = is_input\n",
    "        self.is_output = is_output\n",
    "        \n",
    "        # Z matrix stores the output values for the network\n",
    "        self.Z = np.zeros((minibatch_size, size[0]))\n",
    "        \n",
    "        # the activation fuction is an externally defined function  with a derivative \n",
    "        # that is stored here\n",
    "        self.activation = activation\n",
    "        \n",
    "        # W is the outgoing weight matrix for this layer\n",
    "        self.W = None\n",
    "        \n",
    "        # S is the matrix that holds the inputs for this layer\n",
    "        self.S = None\n",
    "        \n",
    "        # D is the matrix holding the deltas for this layer\n",
    "        self.D  = None\n",
    "        \n",
    "        # Fp is a matrix containin the derivatives of the activation function\n",
    "        self.fp = None\n",
    "        \n",
    "        if not is_input:\n",
    "            self.S = np.zeros((minibatch_size, size[0]))\n",
    "            self.D = np.zeros((minibatch_size, size[0]))\n",
    "        \n",
    "        if not is_output:\n",
    "            self.W = np.random.normal(size=size, scale=1E-4)\n",
    "            \n",
    "        if not is_input and not is_output:\n",
    "            self.Fp = np.zeros((size[0], minibatch_size))\n",
    "\n",
    "    def forward_propagate(self):\n",
    "        if self.is_input:\n",
    "            return self.Z.dot(self.W)\n",
    "\n",
    "        self.Z = self.activation(self.S)\n",
    "        if self.is_output:\n",
    "            return self.Z\n",
    "        else:\n",
    "            # For hidden layers, we add the bias values here\n",
    "            self.Z = np.append(self.Z, np.ones((self.Z.shape[0], 1)), axis=1)\n",
    "            self.Fp = self.activation(self.S, deriv=True).T\n",
    "            return self.Z.dot(self.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layer_config, minibatch_size=100):\n",
    "        self.layers = []\n",
    "        self.num_layers = len(layer_config)\n",
    "        self.minibatch_size = minibatch_size\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            if i == 0:\n",
    "                print( \"Initializing input layer with size {0}.\".format(\n",
    "                    layer_config[i]\n",
    "                ))\n",
    "                # Here, we add an additional unit at the input for the bias\n",
    "                # weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         minibatch_size,\n",
    "                                         is_input=True))\n",
    "            else:\n",
    "                print( \"Initializing hidden layer with size {0}.\".format(\n",
    "                    layer_config[i]\n",
    "                ))\n",
    "                # Here we add an additional unit in the hidden layers for the\n",
    "                # bias weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         minibatch_size,\n",
    "                                         activation=f_sigmoid))\n",
    "\n",
    "        print( \"Initializing output layer with size {0}.\".format(\n",
    "            layer_config[-1]\n",
    "        ))\n",
    "        self.layers.append(Layer([layer_config[-1], None],\n",
    "                                 minibatch_size,\n",
    "                                 is_output=True,\n",
    "                                 activation=f_softmax))\n",
    "        print( \"Done!\")\n",
    "\n",
    "    def forward_propagate(self, data):\n",
    "        # We need to be sure to add bias values to the input\n",
    "        self.layers[0].Z = np.append(data, np.ones((data.shape[0], 1)), axis=1)\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.layers[i+1].S = self.layers[i].forward_propagate()\n",
    "        return self.layers[-1].forward_propagate()\n",
    "\n",
    "    def backpropagate(self, yhat, labels):\n",
    "        self.layers[-1].D = (yhat - labels).T\n",
    "        for i in range(self.num_layers-2, 0, -1):\n",
    "            # We do not calculate deltas for the bias values\n",
    "            W_nobias = self.layers[i].W[0:-1, :]\n",
    "\n",
    "            self.layers[i].D = W_nobias.dot(self.layers[i+1].D) * \\\n",
    "                               self.layers[i].Fp\n",
    "\n",
    "    def update_weights(self, eta):\n",
    "        for i in range(0, self.num_layers-1):\n",
    "            W_grad = -eta*(self.layers[i+1].D.dot(self.layers[i].Z)).T\n",
    "            self.layers[i].W += W_grad\n",
    "\n",
    "    def evaluate(self, train_data, train_labels, test_data, test_labels,\n",
    "                 num_epochs=5, eta=0.05, eval_train=False, eval_test=True):\n",
    "\n",
    "        N_train = len(train_labels)*len(train_labels[0])\n",
    "        N_test = len(test_labels)*len(test_labels[0])\n",
    "\n",
    "        print( \"Training for {0} epochs...\".format(num_epochs))\n",
    "        for t in range(0, num_epochs):\n",
    "            out_str = \"[{0:4d}] \".format(t)\n",
    "\n",
    "            for b_data, b_labels in zip(train_data, train_labels):\n",
    "                output = self.forward_propagate(b_data)\n",
    "                self.backpropagate(output, b_labels)\n",
    "                self.update_weights(eta=eta)\n",
    "\n",
    "            if eval_train:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(train_data, train_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = \"{0} Training error: {1:.5f}\".format(out_str,\n",
    "                                                           float(errs)/N_train)\n",
    "\n",
    "            if eval_test:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(test_data, test_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = \"{0} Test error: {1:.5f}\".format(out_str,\n",
    "                                                       float(errs)/N_test)\n",
    "\n",
    "            print(out_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(layer_config=[784, 100, 100, 10], minibatch_size=minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5 epochs...\n",
      "[   0]  Training error: 0.61094 Test error: 0.61530\n",
      "[   1]  Training error: 0.08876 Test error: 0.07980\n",
      "[   2]  Training error: 0.05556 Test error: 0.05560\n",
      "[   3]  Training error: 0.05538 Test error: 0.05440\n",
      "[   4]  Training error: 0.03196 Test error: 0.03730\n"
     ]
    }
   ],
   "source": [
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels, eval_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -3.07780350e-05,   1.13844859e-04,  -1.23798734e-04, ...,\n",
       "         -2.68208778e-06,  -1.84540163e-04,   5.47450227e-05],\n",
       "       [ -9.88863253e-05,  -1.05963993e-04,   9.05792993e-05, ...,\n",
       "         -1.46710669e-04,   1.58891427e-04,   1.16185031e-04],\n",
       "       [  1.21174084e-04,   1.85879528e-05,   1.33748243e-05, ...,\n",
       "          1.32356735e-04,  -2.43572419e-05,  -2.58250311e-05],\n",
       "       ..., \n",
       "       [  1.11701816e-04,   1.61332145e-07,  -1.13474152e-04, ...,\n",
       "         -3.21694462e-05,   6.76692134e-05,  -8.00345519e-05],\n",
       "       [  6.62623873e-05,   2.52190236e-05,  -4.72522387e-05, ...,\n",
       "         -2.81403650e-04,  -7.16287270e-05,   5.30416326e-05],\n",
       "       [ -1.39383856e+00,  -8.01127133e-01,  -1.12604888e-01, ...,\n",
       "         -3.75319786e-01,  -8.08160311e-01,   1.89651562e-02]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.layers[0].W"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
